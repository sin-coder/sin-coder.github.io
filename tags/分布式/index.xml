<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>分布式 on sin-coder</title>
    <link>https://sin-coder.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/</link>
    <description>Recent content in 分布式 on sin-coder</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Apr 2020 00:53:48 +0800</lastBuildDate>
    
	<atom:link href="https://sin-coder.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>TiDB-调度原理简介</title>
      <link>https://sin-coder.github.io/database/tidbschedu/</link>
      <pubDate>Fri, 24 Apr 2020 00:53:48 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/database/tidbschedu/</guid>
      <description>TiKV是TiDB的数据库的分布式存储引擎，数据以Region为单位进行复制和管理，每个Region为单位进行复制和管理 每个Region会有多个Replica,这些Replica会分布在不同的TiKV节点上，其中leader负责读和写，Follower负责 同步Leader发过来的Raft log
一、问题背景： (1)如何保证同一个Region的多个Replica分布在不同的节点上？一台机器上启动多个TiKV的实例存在的问题？ (2)TiKV集群进行跨机房部署用于容灾的时候，如果一个机房掉线，如何保证不会丢失Raft Group的多个Replica? (3)添加一个节点进入TiKV集群之后，如何将其他节点的数据搬过来？ (4)如果一个节点短暂掉线时如何处理，如果节点长时间掉线，如何处理？ (5)如何调接Replica副本的个数 (6)并不是所有的Region都被经常访问，可能热点数据只在少数的几个Region上 (7)集群在做负载均衡的时候，需要搬迁数据，这种数据的迁移能否不占用大量的网络带宽、磁盘IO和CPU 二、系统调度的需求 1、分布式高可用存储系统的要求： (1)副本数量不能多也不能少 (2)副本需要分布在不同的机器上 (3)新增加节点之后，其他节点上的副本如何迁移过来 (4)节点下线时，需要将该节点的数据迁移走 满足这些需求后系统具有多副本的容错、动态的扩容/缩容、容忍节点掉线和自动故障恢复的功能 2、需要优化的需求： (1)整个集群leader的均匀分布 (2)维持每个节点的存储容量均匀 (3)维持热点分布均匀 (4)管理节点的状态，手动上线/下线节点、自动下线失效节点 满足这些需求后，系统的负载更加均匀，且更加方便的管理 三、系统调度的方案 满足二中的各种需求首先要收集一些信息、比如每个节点的状态，每个Raft Group的消息，业务访问的统计 1、调度的基本操作 (1)增加一个Replica (2)删除一个Replica (3)将leader角色在一个Raft Group的不同Replica转换 恰好，Raft协议能够满足这三种需求，AddReplica、RemoveReplica、TransferLeader三个基本操作丢被支持 2、信息的收集 (1)每个TiKV的节点会定期向PD汇报节点的整体信息 Store与PD直接存在心跳包，一方面 PD通过心跳包检测每个Store是否存活和是否有新加入的Store信息 心跳包中会包含整个节点的Store信息，具体有如下几项： a、总磁盘容量 b、可用磁盘容量 c、承载的Region的数量 d、数据写入的速度 e、发送接收SnapShot的数量，Replica之间会通过SnapShot同步数据额 f.节点是否过载等等 (2)每个Raft的Group的leader会定期向PD汇报信息 每个Raft Group和Leader之间存在心跳包，心跳包的具体信息如下 Leader的位置、Followers的位置、掉线的Replica的数目 数据写入或者读取的速度 3、调度的策略 (1)一个Region的Replica数量正确 当PD通过某个Region Leader的心跳包发现这个Region的Replica数量不满足要求时， 需要通过Add/Remove Replica操作Replica数量,出现这种情况的原因是 a、某个节点掉线，数据丢失，导致一些Region的数量不足 b、某个掉线节点又恢复业务，自动接入集群，但是之前已经补充了Replica的 c、副本策略被调整，修改了最大Replica的配置 (2)一个Raft Group中的多个Replica不在同一个位置 a、多个Replica不会在同一个节点上，避免单个节点的失效导致多个Replica丢失 b、多个节点部署在同一台物理机器上 c、TiKV节点分布在多个机架上，当单个机架掉电时，能够保证系统可用性 d、TiKV节点分布在多个IDC中，在单个机房掉电时，也能保证系统可用 (3)副本在Store之间的均匀分布 副本数量的均衡会使得总体的负载更均衡 (4)Leader数量在Store之间的均匀分布 Raft协议尧要读取和写入都通过Leader进行，计算负载主要在Leader上面，PD尽可能将leader分散开 (5)访问的热点数据在Store时间均匀分布 每个Store以及Region Leader在上报信息的时候携带了当前访问负载的信息，PD会检测出访问热点且将其在节点之间分散开 (6)各Store的存储空间占用大致相同 每个Store启动的时候都会制定一个Capacity参数，表明这个Store的存储空间上限 PD在做调度的时候会考虑节点的剩余空间 (7)控制调度的速度，避免影响正产业务 调度操作比较耗费CPU、内存、磁盘IO和网络带宽。一般情况下会对速度进行控制，当然也可以加快调度的速度 (8)支持手动下线节点 当通过pd-ctl手动下线节点后，PD会在一定的速率控制下，将节点上的数据调度走。当调度完成后 便会将这个节点置为下线状态</description>
    </item>
    
    <item>
      <title>TiDB-存储原理简介</title>
      <link>https://sin-coder.github.io/database/tidbstorage/</link>
      <pubDate>Fri, 24 Apr 2020 00:53:20 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/database/tidbstorage/</guid>
      <description>一、数据库最根本的功能就是把数据存下来，保存数据的方法有很多：
 最简单的就是在内存中直接构建一个数据结构（比如可以使用一个数组），保存用户发来的数据 这个方案非常简单，性能也是非常好的 但是存在很大的缺点：数据完全在内存中，一旦停机或者服务重启，数据就永远丢失 2.为了解决数据丢失的问题，可以将数据存储在非易失性的介质中，比如硬盘 这时我们可以在磁盘上创建一个文件，收到一条数据就在文件中Append一行，这样持久化存储数据 3.但是磁盘会出现坏道，我们可以做RAID,单机冗余存储； 4.但是如果机器挂了呢？我们还可以将存储改为网络存储，或者是通过硬件或者软件进行存储复制，数据安全了， 5.但是复制过程中如何保证数据的一致性呢？也就是在保证数据不丢失的情况下还要保证数据不会出现错误  所以保证数据不丢不错是最基本的要求，但是还有其他的问题：
 跨数据中心的容灾 写入速度如何提高 3.数据保存下来后是否方便读取 4.保存的数据如何修改，如何支持并发的修改？ 5.原子性地修改多条记录 二、TiKV的设计思想 1、TiKV的数据存储模型，是Key-Value的模型，并且提供有序遍历的方法，TiKV的主要特点就是 （1）.TiKV可以看做一个巨大的Map （2）.这个Map中的Key-Value是按照Key的二进制顺序有序排列的，我们可以找到某一个Key的位置 然后不断的调用Next方法以递增的顺序获取比这个Key大的Key-Value 这里的存储模型和SQL中的Table无关，TiKV是一个巨大的分布式Map  2、TiKV数据的持久化机制 TiKV没有选择直接向磁盘上写数据，而是将数据保存在RocksDB中，具体的数据落地由RocksDB完成 RocksDB是一个非常优秀的开源的单机存储引擎 至此TiKV已经有了一个高效可靠的本地存储方案 但是如何保证单机失效的情况下，数据不会丢失和出错，所以我们需要将数据复制到多台机器上，这样一台机器挂了，在其他机器上还有副本，，所以还需要一个可靠，高效且能处理副本失效的情况 3.TiKV的一致性设计思想，优化的Raft算法 Raft的主要就是一个一致性的协议，主要的功能是Leader选举、成员变更、日志复制 TiKv利用Raft来做数据复制，每条数据变更都会落地为一条Raft日志，通过Raft的日志复制功能， 将数据安全可靠地同步到Group的多数节点中 总结下TiKV的设计思想： （1）单机的RocksDB,将数据快速地存储到磁盘上 （2）通过Raft协议，将数据复制到多台机器上，防止单机失效 数据的写入是通过Raft这一层的接口写入，而不是直接写入RocksDB 通过Raft可以实现一个分布式的KV 4、Region 对于一个KV系统，将数据分散在多台机器上一般有两种方案： （1）一种是按照Key做hash,根据hash值选择对应的存储节点 （2）分Range，某一段连续的Key都保存在一个存储节点上 （TiKV解决方案） TiKV将每一个Key-Value空间分成很多段，每一段是一系列连续的Key,我们把每一段成为一个Region 每一个Region会有最大值，目前默认64Mb，每一个Region都可以用StartKey到EndKey这样一个 左开右闭的区间来描述 （3）将数据划分成Region，水平扩展和复制都是以Region为单位的 a、以Region为单位，将数据分散存储在集群的所有节点上，并且尽量保证每个节点上的服务的Region的数量差不多 数据按照Key切分成很多的Region，每个Region的数据只会保存在一个节点上面，系统会有一个组件来负责将Region 尽可能的散布在集群中的所有的节点，一方面实现了存储容量的水平扩展，同时也实现了负载均衡（不会出现某个节点有很多数据 但是其他节点机会没有数据的情况） 系统中会有一个字组件记录Region在节点上面的分布情况，即通过Key可以查到这个Key所处的Region,以及这个Region 目前实在哪个节点上 以Region为单位做Raft的复制和成员的管理 每个Region的数据会保存多个副本，每个副本可称为Replica，Replica之间是通过Raft来保持数据的一致性 一个Region的多个Replica会保存在不同的节点上，构成一个Raft Group，其中一个Replica会作为这个Group 的Leader,其他的Replica作为作为Follower。所有的读和写都会通过Leader进行，再由Leader复制给Follower 总结：以Region为单位做数据的分散和复制，一个具备分布式容灾能力的KeyValue系统就出现雏形了
4、MVCC TiKV的多版本控制 两个Client同时去修改一个Key的Value，如果没有MVCC，就需要对数据上锁，在分布式场景下，可能会导致死锁 TiKV的MVCC实现是通过在Key后面添加Version来实现的， 有了MVCC之后，TiKV的Key排列是这样的，对于同一个Key的多个版本，我们把版本号较大的放在前面，版本号较小的 放在后面，当用户通过一个Key+Version来获取Value时，可以将Key和Version构造出MVCC的Key,也就是Key-Version 然后可以直接Seek（Key-Version），定位到第一个大于等于这个Key-Version的位置 5、TiKV的事务采取的是乐观锁，事务的知行不过程中不会检测写冲突，只有在提交的过程中，才会做冲突检测，冲突的双方 中比较早完成提交的会写入成功，另一方会重新尝试执行整个事务
如何理解分布式事务？ 首先应该看看事务是如何定义的： 事务提供一种机制将一个活动所涉及的所有操作纳入到一个不可分割的执行单元，组成事务的所有操作 只有在所有操作能正常执行的情况下方能提交，只要其中任一操作执行失败，都将导致整个事务的回滚 简单的说事务就是要么什么都不做，要么做全套，All OR Nothing</description>
    </item>
    
    <item>
      <title>TiDB-计算原理</title>
      <link>https://sin-coder.github.io/database/tidbcompute/</link>
      <pubDate>Fri, 24 Apr 2020 00:53:05 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/database/tidbcompute/</guid>
      <description>问题背景：如何在KV结构上保存Table以及如何在KV结构上运行SQL语句
一、SQL是如何被映射成KV存储结构的 1、原理： TiDB的存储引擎是一个全局有序的分布式Key-Value引擎 TiDB对于每一个表分配一个TableID,每一个索引都会分配一个IndexID,每一行分配一个RowID TableID在整个集群中唯一，IndexID/RowID在表内唯一，这些ID都是int64类型 2、具体实现： （1）每行数据按照如下规则进行编码， Key:tablePrefix{tableID}_recordPrefixSep{rowID} Value:[col1,col2,col3,col4] 其中tablePrefix和recordPrefixSep都是特定的字符串常量，用于在KV空间中 区分其他的数据 （2）对于Unique Index数据，按照如下规则进行编码： Key:tablePrefix{tableID}_indexPrefixSep{indexID}_indexedColumnsValue (3)对于非Unique Index，可能有多行数据的ColumnsValue一样，所以应该这样去编码： Key:tablePrefix{tableID}_indexPrefixSep{IndexID}_indexedColumnsValue_rowID Value:null 对于上述编码规则中的Key里面的Prefix都是字符串常量，作用都是区分命名空间，以避免不同类型数据之间的相互冲突 而且一个Table内部的所有Row都有相同的前缀，一个Index的数据也是有相同的数据的 所以可以非常方便的将Row或者Index数据有序的保存在TiKV中，即一个表中的所有Row数据就会按照RowID的顺序排列在TiKV的Key空间中 某一个Index的数据也会按照Index的顺序排列在Key空间内 3、元信息的管理 Database/Table都有元信息，就是表的定义和各项属性，这些信息需要持久化的存储在TiKV中，每个Database/Table都被分配了 一个唯一的ID。这个ID作为唯一的标识，并且在编码为Key-Value，这个ID都会编码到Key中。这样可以构造出一个Key信息，Value存储的是 序列化后的元信息。除此之外，还有一个专门的Key-Value来存储当前的Schema信息
二、TiDB的整体结构 1、TiKV Cluster主要的作用就是作为KV引擎存储数据 2、TiDB Servers这一层就是无状态的节点，本身并不会去存储数据，节点之间完全对等； TiDB Server这一层主要是处理用户的请求，执行SQL逻辑运算
三、SQL层运算 各种数据库操作语句是如何操作底层数据的？将SQL查询映射为KV的查询，再通过KV的接口获取对应的数据 1、查询方案的简介： 以select count(*) from user where name = &amp;quot;TiDB&amp;quot;为例 （1）构造Key Range:一个表中所有的RowID都在[0,MaxInt64]这个范围内，那么我们可以根据Row Key编码规则，构造出一个[StartKey,EndKey]这样的左开右闭的区间 （2）根据上面构造出的Key Range。读取TiKV中的数据 （3）过滤数据，对于读取到的每一行的数据，计算name = &amp;quot;TiDB&amp;quot;这个表达式。，如果为真，向上返回这一行，否则丢弃这一行 （4）根据返回的行数计算Count值
总结该方案的缺点： （1）从TiKV中读取每一行数据时都要进行一次扫描，但是每次扫描都是一次RPC调用，当扫描的数据很多时，这个开销非常大 （2）不满足条件的行可以不用读取 （3）只需统计行数即可 2、方案的改进：分布式SQL的运算
将计算、Filter、聚合函数、GroupBy尽量地靠近存储节点，以避免大量的RPC调用 数据逐层返回的示意图如下： 3、SQL层的架构 TiDB的SQL层是非常复杂的 （1）用户的请求会直接或者通过Load Balancer发送到TiDB-Server, （2）TiDB-Server会解析MySQL Protocol Packet，获取请求的内容，然后左语法解析、查询特定和优化，执行查询计划和处理数据 (3) 数据全部存储在TiKV的集群中，TiDB和TiKV-Server交互，获取数据 （4）最后TiDB-Server需要将特定的查询结果返回给特定的用户</description>
    </item>
    
    <item>
      <title>TiDB架构原理简介</title>
      <link>https://sin-coder.github.io/database/tidbinfr/</link>
      <pubDate>Fri, 24 Apr 2020 00:52:37 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/database/tidbinfr/</guid>
      <description>TiDB集群主要包括三个核心组件：TiDB Server,PD Server,TiKV Server TiSpark组件(解决用户复杂OLAP需求)和TiDB Operator组件(简化云上部署管理) 架构简介: 一、TiDB Server TiDB Server 负责接收SQL请求，处理SQL相关的逻辑，并通过PD找到存储计算所需数据的TiKV地址，与TiKV交互数据，最终返回结果。 TiDB Server 是无状态的，其本身并不存储数据，只负责计算，可以无限水平扩展，可通过负载均衡的组件(LVS、HAProxy或者F5) 对外提供统一的接入地址 二、PD Server(Placement Driver)是整个集群的管理模块，具体的工作职能是： 1、存储这个集群的原信息(某个Key具体存储在哪个TiKV的节点) 2、对TiKV集群进行调度和负载均衡(数据迁移、Raft Group leader) 3、分配全局唯一且递增的事物ID PD通过Raft协议保证数据的安全性,Raft的leader Server负责处理所有操作，其余的PD Server仅用于保证高可用性 一般情况下建议部署奇数个节点 三、TiKV Server TiKV Server是TiDB中的数据存储引擎，存储的数据的基本单位是Region TiKV使用Raft做协议复制，保证数据的一致性和容灾。副本以Region为单位进行管理，不同节点上的多个Region构成一个Raft Group，互为副本 数据在多个TiKV之间的负载均衡由PD调度，也是以Region为基本单位的
四、TiSpark 主要应用在OLTP和OLAP中 五、TiDB Opeartor 提供在主流云基础设施上部署管理TiDB集群的能力 集成一键部署、多集群混部、自动运维、故障自愈  特性分析： TiDB的两大核心特点就是无限水平扩展和高可用性 一、无限水平扩展 水平扩展包括计算能力和存储能力。TiDB Server负责处理Server的请求，可以增加TiDB Server节点，提高计算能力 TiKV负责存储数据，增加TiKV可以增加存储能力 二、高可用性 TiDB、TiKV和PD这三个组件都能容忍部分实例失效，不影响整个集群的可用性; 1、TiDB TiDB是无状态的,应该部署至少两个实例，前端通过负载均衡的组件对外提供服务 如果单次请求失败后，重新连接就可继续获得服务 2、PD PD是一个集群，通过Raft协议保持数据的一致性，当单个实例失效时，如果不是Raft的Leader，完全不影响服务 如果是Raft的Leader，会重新选举出Raft的leader,自动恢复服务 但是PD在选举的过程中无法对外提供服务 3、TiKV 也是一个集群，通过Raft协议保证数据的一致性，副本数量可配置，默认保持三副本，通过PD做负载均衡调度。 单个节点失效时，会影响这个节点上存储的所有Region。 对于Region中的Leader节点，会中断服务，等待重新选举；对于Follower节点，不会影响服务 如果某个TiKV节点失效时，并且在一段时间内无法恢复，PD会将其上的数据迁移到其他的TiKV节点上</description>
    </item>
    
    <item>
      <title>TiDB数据库简介</title>
      <link>https://sin-coder.github.io/database/tidbintro/</link>
      <pubDate>Fri, 24 Apr 2020 00:51:14 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/database/tidbintro/</guid>
      <description>TiDB主要用来应用在全部的OLTP的场景和大部分的OLAP场景 TiDB作为一个开源的分布式数据库主要有以下特点： 1. 高度兼容MySQL MySQL迁移到TiDB是非常容易的，无论是单机迁移还是集群的迁移 2. 支持无限的水平弹性扩展 新增节点实现TiDB的水平扩展，轻松地应对高并发、海量数据的场景 3. 强一致性和高可用 基于Raft的多数派选举协议可以提供100%的数据强一致性，可以实现故障的自动恢复 4. 分布式事务 支持ACID事务</description>
    </item>
    
    <item>
      <title>分布式一致性</title>
      <link>https://sin-coder.github.io/post/consistency/</link>
      <pubDate>Mon, 17 Feb 2020 08:04:41 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/post/consistency/</guid>
      <description>分布式一致性概述 一、什么是分布式一致性 1.CAP 理论  对于分布式一致性，最直观的理解就是分布式系统中的不同节点不能产生矛盾。比较著名的理论就是
 CAP Theorem，即在一个分布式系统中，不能同时满足以下三点：一致性（Consistency）、可用性
（Availability）、分区容错性（Partition Tolerance）
 一致性（C）：在分布式系统中的所有数据备份，同一时刻是否有同样的值
 可用性（A）：在集群中一部分节点故障后，集群整体能否响应客户端的读写请求
 分区容忍性（P）：大多数的分布式系统都分布在多个子网络，每个网络都叫做一个区，分区容错的意思即是
    区间通信可能失败；比如一个分布式系统有5个节点，有3个在美国，有两个在中国，这就是两个区
它们之间可能无法通信
   CAP原则的核心就是只能实现AP、CP、AC，不会存在CAP，从上图中也可以看到典型的一些数据库
 产品也只是满足了CAP的部分特性
2.一致性模型  （1）弱一致性（最终一致性）
关于弱一致性，通俗的解释就是当一个节点向数据库写入数据时，其他的节点可能无法立即读到该数据，
 但是它们最终一定会读到该数据，下面是一些典型的实例
 DNS （Domain Name System）
 Gossip（Cassandra 的通讯协议）
   （2）强一致性
对于分布式系统的容错性最关注的问题就是数据不能存储在单个的节点上，一般的解决方案就是state
 machine replication（状态机复制共识算法）,具体的实现算法有以下几种：
 同步
 Paxos
 Raft（multi-paxos）
 ZAB（multi-paxos）
  二、强一致性算法 1.主从同步  主从同步复制的工作过程如下，Master接受写请求、Master复制日志到slave、Master等待，直到
 所有从库返回；但是这样存在一个问题：一个节点失败，Master阻塞，导致整个集群不可用，保证了
一致性，但是可用性却大大降低了
 解决上述问题的方法：多数派的算法，每次写都保证写入大于N/2个节点，每次读保证从大于N/2个</description>
    </item>
    
    <item>
      <title>消息队列简介</title>
      <link>https://sin-coder.github.io/post/messagequ/</link>
      <pubDate>Tue, 28 Jan 2020 01:22:22 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/post/messagequ/</guid>
      <description>消息队列简介 一、消息队列简介 1.概述  消息是指在应用间传送数据，消息可以只包含文本字符串、或者包含嵌入式对象。
消息队列是一种应用程序对应用程序的通信方法。它是是生产者-消费者模型的一个典型的代表，一端往消息
 队列中不断地写入消息，而另一端则可以读取队列中的消息。这样发布者和接受者都不知道对方的存在。
 消息队列也可以简单理解为：把要传输的数据放在队列中
 2.消息获取模式  消费者获取消息时有两种模式，点对点模式和发布订阅模式
 （1）点对点模式  点对点模型通常是一个基于拉取或者轮询的消息传递模型，这种模型从队列中请求消息，而不是将消息推送
 到客户端。这种模式的特点是一对一，发送到队列的消息被一个且只有一个接受者接收处理，即使有多
个消息监听者也是如此，消息被收到后即可清除
 点对点模式的优点是队列发送数据和客户端接收数据的速度是相匹配的，缺点是客户端需要实时监控队列中
 是否有消息存在
（2）发布/订阅模式  发布订阅模型是一个基于推送的消息传送模型，该种模型下订阅者有临时订阅者和持久订阅者之分，临时订
 阅者只在主动监听主题时才接收消息；而持久订阅者则监听主题的所有消息，即使当前订阅者不可用，
处于离线状态。这种模型的特点是一对多，数据生产后，推送给所有的订阅者
 发布/订阅模式的优点是客户端不需要实时监控队列中是否有消息存在，缺点是队列发送数据的速度无法和多
 个客户端接收数据的速度是相匹配
二、消息队列作用  解耦：客户端与客户端之间或者客户端和服务器 之间不需要直接连接，而是通过中间件来进行连接。而且允许你    独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束
  冗余：消息队列可以对数据进行持久化（本地备份）直到它们已经被处理，这样就规避了数据的丢失。许多消息   队列均采用“插入-获取-删除”的范式，即在把一个消息从队列中删除之前，需要你的系统明确的指出该消息已
经被处理完毕
  峰值处理：可以组建集群，进而增大消息入队和处理的频率。在访问量剧增的情况下，应用仍然需要继续发挥作   用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的
浪费。消息队列基于它的可扩展性使其本身可以顶住突发的访问压力，而不会因为突发的超负荷的请求而使
系统完全崩溃
  数据可恢复：当系统的一部分组件失效时，不会影响到整个的系统 。消息队列降低了进程间的耦合度，即使一个   处理消息的进程挂掉，加入队列中的消息仍可在系统恢复后被处理
  顺序保证：消息队列本来就是排好序的，并且也能够保证数据按照特定的顺序来进行处理
 缓冲：消息队列可以控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况</description>
    </item>
    
    <item>
      <title>Git&amp;Github 学习笔记</title>
      <link>https://sin-coder.github.io/post/git/</link>
      <pubDate>Mon, 27 Jan 2020 00:52:28 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/post/git/</guid>
      <description>一、版本控制 1.版本控制工具的功能  协同修改：多人互不影响地修改服务器端的同一个文件
 数据备份：不仅要保存文件的当前状态，还能够保存每一个提交过的历史状态
 版本管理：在保存每一个版本的文件信息时，能够做到不保存重复的数据，节省存储空间，提高运行效率;
     SVN采取的是增量式管理的方式，Git采取了文件系统快照的方式
    权限控制：对团队中参与开发的人员进行权限控制，Git还可以对团队外开发者贡献的代码进行审核
 历史记录：查看修改人、修改时间、修改内容、日志信息；将本地文件恢复至某一个历史状态
 分支管理：允许开发团队在工作过程中多条生产线同时推进任务，提高效率
  2.常见版本控制工具  （1）集中式版本控制工具：CVS、SVN、VSS等
 集中式的版本控制中每个开发者是一个客户端，文件和版本信息存储在服务端，开发者们都直接与
 服务器进行交互，集中式的版本控制具有单点故障的问题
（2）分布式版本控制工具：Git、Mercurial、Bazaar等
 分布式版本控制相比于集中式最大的优点就是能够避免单点故障的问题
  二、Git 简介 1.Git的发展历史  Git是一个免费、开源的分布式版本控制工具。在2005年，由Linus基于C语言开发完成，开发的初衷是
 管理Linux社区中提交的代码, 而这位Linus正是是开发Linux系统内核的大神，它的个人语录也是我的座右铭
&amp;quot;Talk is cheap, Show me the Code&amp;quot;，少废话我只看代码。
2.Git的特性简介  从Git的图标中就可以看到分支是其最引以为傲的特点，实际上Git的优点还有很多
  大部分操作在本地完成版本控制，不需要联网
 对数据进行完整性保证，基于Hash算法
 尽可能添加数据而不是删除或者修改数据
 与Linux命令全面兼容，这个当然了，都是由Linus开发的
    3.Git 的结构  Git的本地结构图</description>
    </item>
    
    <item>
      <title>分布式系统学习笔记</title>
      <link>https://sin-coder.github.io/post/distri/</link>
      <pubDate>Mon, 27 Jan 2020 00:44:13 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/post/distri/</guid>
      <description>一、分布式系统概述 1.什么是分布式系统？ ​ 分布式系统主要由网络、分布式存储与分布式计算等部分构成的，分布式存储侧重于数据的读写存取及一致性等方面，而分布式计算则侧重于资源、任务的编排和调度
2.分布式系统的特点 ​ 没有强制性的中心控制、次级单位具有自治的特质、次级单位之间彼此高度链接、点对点之间的影响通过网络形成了非线性的因果关系
3.传统架构面临的难题: 系统的扩展 ​ 高并发的访问要求我们的后端系统架构弹性且可扩展
​ 三维扩展：
​ X轴扩展：水平复制，即在负载均衡服务器后增加多个Web服务器，
​ Y轴扩展：对数据库的扩展，即进行分库分表，分库是将关系紧密的表放在一台数据库服务器上，分表是因为一张表的数据太多，需要将一张表的数据通过hash放在不同的数据库服务器上
​ Z轴扩展：业务方向的扩展，才能将巨型应用分解为一组不同的服务，将应用进一步分解为微服务
​ 4.CAP定理
​ 在分布式系统中，系统的一致性(Consistency)、可用性（Availability）、分区容忍性(Partion tolerance)。这三者不能同时保证，由于网络通信的不确定性，分区的容忍性是必须要保证的，而且互联网应用比企业级应用更加偏向于保持可用性，通常用最终一致性代替传统事务的ACID强一致性
​
二、分布式计算 1.概述 ​ 分布式计算核心的思路就是系统架构无单点，让整个系统可以扩展。分布式计算环境下的节点分为有状态存储节点和无状态存储节点。
​ 无状态存储节点，不存储数据，请求分发可以采取很简单的随机算法或者是轮询的算法就可以了，如果需要增加机器，则只需要把对应的运算代码部署到一些机器上然后启动起来，引导流量到那些机器即可实现动态的扩展了。简单来说就是某台机器承担了某种角色后，能够快速的广播给需要这个角色提供服务的机器。
​ 而针对有状态节点，扩容难度较大，因为每台Server中均有数据，所以请求分发的算法不能够随机或者轮询，一般来说常见算法就是哈希或者使用Tree来做一层映射，增加机器时需要经历一个复杂的数据迁移过程------》自动化扩容和迁移的工具
2.数据处理的发展过程
GFS-------------》HDFS
BigTable--------》HBase
​ MapReduce----》MapReduce
​ （Hadoop技术栈）
MapReduce(离线处理)-----》Spark(高性能批处理技术)------》Storm(流处理)----》Flink
3.批处理（Batch Processing）与流处理（Stream Processing） 主要区别：每一条数据在到达时是被处理的（流处理），还是作为一组新数据的一部分稍后进行处理（批处理）
批处理：在批处理中新到达的数据元素被收集到一个组中，整个组在未来的时间内进行处理。至于何时处理每个组可以选择多种方式来确定，可以基于预定的时间间隔（如每隔5分钟）、或者在某些触发的条件下（只要包含5个元素/拥有超过1MB的数据）。传统的数据仓库和Hadoop就是专注于批处理的。批处理示意图如下：
缺点：具有延迟性、新数据的到达与该数据的处理之间的延迟将取决于直到下一批处理窗口的时间
流处理：流处理设计的目的是为了在数据到达时对其进行响应，这就要求它们实现一个由事件驱动的体系架构，也可以说是在系统的内部工作流在接收到数据后立即连续监视新数据和调度处理。
应用：Flink、Beam等都支持“流式处理优先，将批处理视为流式处理的特殊情况”，但是流式处理器的出现并没有让批处
​ 理器变得过时。因为纯流式处理系统在批处理工作负载时其实是非常慢的。
​ Apache Beam: 这样统一的API通常会根据数据是持续的（无界）、还是固定的（有界）将工作负载委托给不同的
​ 运行机制
​ Flink: 提供的流式API，可以处理有界或者无界的场景，同时任然提供了单独的DataSet API用于批处理
​
三、分布式调度 1.概述
经典资源调度器（Yarn）-----》数据调度（Data Placement）、资源任务调度（Resource Management）、计算调度（Application Manager）、本地微（自治）调度
2.资源调度</description>
    </item>
    
    <item>
      <title>同步/异步、阻塞/非阻塞辨析</title>
      <link>https://sin-coder.github.io/post/syn/</link>
      <pubDate>Sun, 26 Jan 2020 21:35:45 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/post/syn/</guid>
      <description>关键词：同步、异步、阻塞、非阻塞 相关概念：网络编程、进程与线程、I/O模型 一、问题背景  同步和异步，以及阻塞和非阻塞都是网络编程中经常遇到的概念，单看文字上的解释确实有些晦涩
 难懂。接下来我们将从一个通俗的例子出发阐述它们的区别与联系
二、一个简单的例子  隔壁老王爱好茶艺，每天都会煮开水来泡茶
 场景一：老王将水壶放在火上，坐在旁边等待水开 （同步阻塞）
 但是这样很耽搁时间，又不自由，效率很低，老王想换种方法
 场景二：老王将水壶放在火上，自已去隔壁了，每隔3分钟来看下水开没有 （同步非阻塞）
 但是这样依旧很麻烦，老王就买了一个自动报警的水壶
 场景三： 老王用新买的水壶进行烧水，坐在旁边等待水开 （异步阻塞）
 老王便想没有必要在水壶旁边坐着啊
 场景四： 老王新买的水壶放在火上，自己去隔壁了，等着报警再回来 （异步非阻塞）
 这种方式是最让老王省心的
 小结： 同步和异步关注的焦点在于我们是否需要不断地去看水壶是否开了，同步时，需要老王不断
地去轮询水壶是否开了，效率是比较低下的。而异步时，水壶告警提醒老王它开了
 阻塞和非阻塞 关注的焦点在于老王是否需要坐在水壶旁边等待，在水壶旁边等待老王就是阻
 塞的，去做其他事的老王就是非阻塞的
 这个例子可以帮助我们初步地理解同步异步、阻塞和非阻塞之间的联系和区别，但是如果详细
 的“追究”起来，还有许多未解释的细节
三、理论阐述 1.同步与异步  同步和异步（syn &amp;amp; asyn），描述的是在单线程中一次方法调用后，执行者是否具备主动通知
 的功能。同步时调用者会等到方法调用返回后才能继续后面的行为，异步时调用者不需要等到方法返回，
方法执行完毕后会主动通知调用者
2.阻塞和非阻塞  阻塞和非阻塞关注是调用者是否可以执行多个任务，描述的是调用者的多个线程是否可以同时执
 行。阻塞时，多个线程不能同时进行；非阻塞时，多个线程可以同时进行
3.二者的区别与联系  同步和阻塞完全是在单线程和多线程这两个维度上的概念，它们之间并没有强制的联系。但是从
 实际的意义来看确实有一定的绑定关系，比如对于单线程来说，不管是同步还是异步，肯定是阻塞的，非
阻塞只有多线程而且异步的时候才能发挥作用。
 回来继续看烧水的例子，老王在烧水的同时去隔壁，也即在烧水这个线程之中，又开启了去隔壁
 这个线程，所以使用异步非阻塞才更加有意义</description>
    </item>
    
  </channel>
</rss>