<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Databases on sin-coder</title>
    <link>https://sin-coder.github.io/database/</link>
    <description>Recent content in Databases on sin-coder</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Apr 2020 14:25:08 +0800</lastBuildDate>
    
	<atom:link href="https://sin-coder.github.io/database/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>MDB平台操作中常见的概念/服务平台简介</title>
      <link>https://sin-coder.github.io/database/newconcept/</link>
      <pubDate>Fri, 24 Apr 2020 14:25:08 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/database/newconcept/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TiDB-调度原理简介</title>
      <link>https://sin-coder.github.io/database/tidbschedu/</link>
      <pubDate>Fri, 24 Apr 2020 00:53:48 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/database/tidbschedu/</guid>
      <description>TiKV是TiDB的数据库的分布式存储引擎，数据以Region为单位进行复制和管理，每个Region为单位进行复制和管理 每个Region会有多个Replica,这些Replica会分布在不同的TiKV节点上，其中leader负责读和写，Follower负责 同步Leader发过来的Raft log
一、问题背景： (1)如何保证同一个Region的多个Replica分布在不同的节点上？一台机器上启动多个TiKV的实例存在的问题？ (2)TiKV集群进行跨机房部署用于容灾的时候，如果一个机房掉线，如何保证不会丢失Raft Group的多个Replica? (3)添加一个节点进入TiKV集群之后，如何将其他节点的数据搬过来？ (4)如果一个节点短暂掉线时如何处理，如果节点长时间掉线，如何处理？ (5)如何调接Replica副本的个数 (6)并不是所有的Region都被经常访问，可能热点数据只在少数的几个Region上 (7)集群在做负载均衡的时候，需要搬迁数据，这种数据的迁移能否不占用大量的网络带宽、磁盘IO和CPU 二、系统调度的需求 1、分布式高可用存储系统的要求： (1)副本数量不能多也不能少 (2)副本需要分布在不同的机器上 (3)新增加节点之后，其他节点上的副本如何迁移过来 (4)节点下线时，需要将该节点的数据迁移走 满足这些需求后系统具有多副本的容错、动态的扩容/缩容、容忍节点掉线和自动故障恢复的功能 2、需要优化的需求： (1)整个集群leader的均匀分布 (2)维持每个节点的存储容量均匀 (3)维持热点分布均匀 (4)管理节点的状态，手动上线/下线节点、自动下线失效节点 满足这些需求后，系统的负载更加均匀，且更加方便的管理 三、系统调度的方案 满足二中的各种需求首先要收集一些信息、比如每个节点的状态，每个Raft Group的消息，业务访问的统计 1、调度的基本操作 (1)增加一个Replica (2)删除一个Replica (3)将leader角色在一个Raft Group的不同Replica转换 恰好，Raft协议能够满足这三种需求，AddReplica、RemoveReplica、TransferLeader三个基本操作丢被支持 2、信息的收集 (1)每个TiKV的节点会定期向PD汇报节点的整体信息 Store与PD直接存在心跳包，一方面 PD通过心跳包检测每个Store是否存活和是否有新加入的Store信息 心跳包中会包含整个节点的Store信息，具体有如下几项： a、总磁盘容量 b、可用磁盘容量 c、承载的Region的数量 d、数据写入的速度 e、发送接收SnapShot的数量，Replica之间会通过SnapShot同步数据额 f.节点是否过载等等 (2)每个Raft的Group的leader会定期向PD汇报信息 每个Raft Group和Leader之间存在心跳包，心跳包的具体信息如下 Leader的位置、Followers的位置、掉线的Replica的数目 数据写入或者读取的速度 3、调度的策略 (1)一个Region的Replica数量正确 当PD通过某个Region Leader的心跳包发现这个Region的Replica数量不满足要求时， 需要通过Add/Remove Replica操作Replica数量,出现这种情况的原因是 a、某个节点掉线，数据丢失，导致一些Region的数量不足 b、某个掉线节点又恢复业务，自动接入集群，但是之前已经补充了Replica的 c、副本策略被调整，修改了最大Replica的配置 (2)一个Raft Group中的多个Replica不在同一个位置 a、多个Replica不会在同一个节点上，避免单个节点的失效导致多个Replica丢失 b、多个节点部署在同一台物理机器上 c、TiKV节点分布在多个机架上，当单个机架掉电时，能够保证系统可用性 d、TiKV节点分布在多个IDC中，在单个机房掉电时，也能保证系统可用 (3)副本在Store之间的均匀分布 副本数量的均衡会使得总体的负载更均衡 (4)Leader数量在Store之间的均匀分布 Raft协议尧要读取和写入都通过Leader进行，计算负载主要在Leader上面，PD尽可能将leader分散开 (5)访问的热点数据在Store时间均匀分布 每个Store以及Region Leader在上报信息的时候携带了当前访问负载的信息，PD会检测出访问热点且将其在节点之间分散开 (6)各Store的存储空间占用大致相同 每个Store启动的时候都会制定一个Capacity参数，表明这个Store的存储空间上限 PD在做调度的时候会考虑节点的剩余空间 (7)控制调度的速度，避免影响正产业务 调度操作比较耗费CPU、内存、磁盘IO和网络带宽。一般情况下会对速度进行控制，当然也可以加快调度的速度 (8)支持手动下线节点 当通过pd-ctl手动下线节点后，PD会在一定的速率控制下，将节点上的数据调度走。当调度完成后 便会将这个节点置为下线状态</description>
    </item>
    
    <item>
      <title>TiDB-存储原理简介</title>
      <link>https://sin-coder.github.io/database/tidbstorage/</link>
      <pubDate>Fri, 24 Apr 2020 00:53:20 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/database/tidbstorage/</guid>
      <description>一、数据库最根本的功能就是把数据存下来，保存数据的方法有很多：
 最简单的就是在内存中直接构建一个数据结构（比如可以使用一个数组），保存用户发来的数据 这个方案非常简单，性能也是非常好的 但是存在很大的缺点：数据完全在内存中，一旦停机或者服务重启，数据就永远丢失 2.为了解决数据丢失的问题，可以将数据存储在非易失性的介质中，比如硬盘 这时我们可以在磁盘上创建一个文件，收到一条数据就在文件中Append一行，这样持久化存储数据 3.但是磁盘会出现坏道，我们可以做RAID,单机冗余存储； 4.但是如果机器挂了呢？我们还可以将存储改为网络存储，或者是通过硬件或者软件进行存储复制，数据安全了， 5.但是复制过程中如何保证数据的一致性呢？也就是在保证数据不丢失的情况下还要保证数据不会出现错误  所以保证数据不丢不错是最基本的要求，但是还有其他的问题：
 跨数据中心的容灾 写入速度如何提高 3.数据保存下来后是否方便读取 4.保存的数据如何修改，如何支持并发的修改？ 5.原子性地修改多条记录 二、TiKV的设计思想 1、TiKV的数据存储模型，是Key-Value的模型，并且提供有序遍历的方法，TiKV的主要特点就是 （1）.TiKV可以看做一个巨大的Map （2）.这个Map中的Key-Value是按照Key的二进制顺序有序排列的，我们可以找到某一个Key的位置 然后不断的调用Next方法以递增的顺序获取比这个Key大的Key-Value 这里的存储模型和SQL中的Table无关，TiKV是一个巨大的分布式Map  2、TiKV数据的持久化机制 TiKV没有选择直接向磁盘上写数据，而是将数据保存在RocksDB中，具体的数据落地由RocksDB完成 RocksDB是一个非常优秀的开源的单机存储引擎 至此TiKV已经有了一个高效可靠的本地存储方案 但是如何保证单机失效的情况下，数据不会丢失和出错，所以我们需要将数据复制到多台机器上，这样一台机器挂了，在其他机器上还有副本，，所以还需要一个可靠，高效且能处理副本失效的情况 3.TiKV的一致性设计思想，优化的Raft算法 Raft的主要就是一个一致性的协议，主要的功能是Leader选举、成员变更、日志复制 TiKv利用Raft来做数据复制，每条数据变更都会落地为一条Raft日志，通过Raft的日志复制功能， 将数据安全可靠地同步到Group的多数节点中 总结下TiKV的设计思想： （1）单机的RocksDB,将数据快速地存储到磁盘上 （2）通过Raft协议，将数据复制到多台机器上，防止单机失效 数据的写入是通过Raft这一层的接口写入，而不是直接写入RocksDB 通过Raft可以实现一个分布式的KV 4、Region 对于一个KV系统，将数据分散在多台机器上一般有两种方案： （1）一种是按照Key做hash,根据hash值选择对应的存储节点 （2）分Range，某一段连续的Key都保存在一个存储节点上 （TiKV解决方案） TiKV将每一个Key-Value空间分成很多段，每一段是一系列连续的Key,我们把每一段成为一个Region 每一个Region会有最大值，目前默认64Mb，每一个Region都可以用StartKey到EndKey这样一个 左开右闭的区间来描述 （3）将数据划分成Region，水平扩展和复制都是以Region为单位的 a、以Region为单位，将数据分散存储在集群的所有节点上，并且尽量保证每个节点上的服务的Region的数量差不多 数据按照Key切分成很多的Region，每个Region的数据只会保存在一个节点上面，系统会有一个组件来负责将Region 尽可能的散布在集群中的所有的节点，一方面实现了存储容量的水平扩展，同时也实现了负载均衡（不会出现某个节点有很多数据 但是其他节点机会没有数据的情况） 系统中会有一个字组件记录Region在节点上面的分布情况，即通过Key可以查到这个Key所处的Region,以及这个Region 目前实在哪个节点上 以Region为单位做Raft的复制和成员的管理 每个Region的数据会保存多个副本，每个副本可称为Replica，Replica之间是通过Raft来保持数据的一致性 一个Region的多个Replica会保存在不同的节点上，构成一个Raft Group，其中一个Replica会作为这个Group 的Leader,其他的Replica作为作为Follower。所有的读和写都会通过Leader进行，再由Leader复制给Follower 总结：以Region为单位做数据的分散和复制，一个具备分布式容灾能力的KeyValue系统就出现雏形了
4、MVCC TiKV的多版本控制 两个Client同时去修改一个Key的Value，如果没有MVCC，就需要对数据上锁，在分布式场景下，可能会导致死锁 TiKV的MVCC实现是通过在Key后面添加Version来实现的， 有了MVCC之后，TiKV的Key排列是这样的，对于同一个Key的多个版本，我们把版本号较大的放在前面，版本号较小的 放在后面，当用户通过一个Key+Version来获取Value时，可以将Key和Version构造出MVCC的Key,也就是Key-Version 然后可以直接Seek（Key-Version），定位到第一个大于等于这个Key-Version的位置 5、TiKV的事务采取的是乐观锁，事务的知行不过程中不会检测写冲突，只有在提交的过程中，才会做冲突检测，冲突的双方 中比较早完成提交的会写入成功，另一方会重新尝试执行整个事务
如何理解分布式事务？ 首先应该看看事务是如何定义的： 事务提供一种机制将一个活动所涉及的所有操作纳入到一个不可分割的执行单元，组成事务的所有操作 只有在所有操作能正常执行的情况下方能提交，只要其中任一操作执行失败，都将导致整个事务的回滚 简单的说事务就是要么什么都不做，要么做全套，All OR Nothing</description>
    </item>
    
    <item>
      <title>TiDB-计算原理</title>
      <link>https://sin-coder.github.io/database/tidbcompute/</link>
      <pubDate>Fri, 24 Apr 2020 00:53:05 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/database/tidbcompute/</guid>
      <description>问题背景：如何在KV结构上保存Table以及如何在KV结构上运行SQL语句
一、SQL是如何被映射成KV存储结构的 1、原理： TiDB的存储引擎是一个全局有序的分布式Key-Value引擎 TiDB对于每一个表分配一个TableID,每一个索引都会分配一个IndexID,每一行分配一个RowID TableID在整个集群中唯一，IndexID/RowID在表内唯一，这些ID都是int64类型 2、具体实现： （1）每行数据按照如下规则进行编码， Key:tablePrefix{tableID}_recordPrefixSep{rowID} Value:[col1,col2,col3,col4] 其中tablePrefix和recordPrefixSep都是特定的字符串常量，用于在KV空间中 区分其他的数据 （2）对于Unique Index数据，按照如下规则进行编码： Key:tablePrefix{tableID}_indexPrefixSep{indexID}_indexedColumnsValue (3)对于非Unique Index，可能有多行数据的ColumnsValue一样，所以应该这样去编码： Key:tablePrefix{tableID}_indexPrefixSep{IndexID}_indexedColumnsValue_rowID Value:null 对于上述编码规则中的Key里面的Prefix都是字符串常量，作用都是区分命名空间，以避免不同类型数据之间的相互冲突 而且一个Table内部的所有Row都有相同的前缀，一个Index的数据也是有相同的数据的 所以可以非常方便的将Row或者Index数据有序的保存在TiKV中，即一个表中的所有Row数据就会按照RowID的顺序排列在TiKV的Key空间中 某一个Index的数据也会按照Index的顺序排列在Key空间内 3、元信息的管理 Database/Table都有元信息，就是表的定义和各项属性，这些信息需要持久化的存储在TiKV中，每个Database/Table都被分配了 一个唯一的ID。这个ID作为唯一的标识，并且在编码为Key-Value，这个ID都会编码到Key中。这样可以构造出一个Key信息，Value存储的是 序列化后的元信息。除此之外，还有一个专门的Key-Value来存储当前的Schema信息
二、TiDB的整体结构 1、TiKV Cluster主要的作用就是作为KV引擎存储数据 2、TiDB Servers这一层就是无状态的节点，本身并不会去存储数据，节点之间完全对等； TiDB Server这一层主要是处理用户的请求，执行SQL逻辑运算
三、SQL层运算 各种数据库操作语句是如何操作底层数据的？将SQL查询映射为KV的查询，再通过KV的接口获取对应的数据 1、查询方案的简介： 以select count(*) from user where name = &amp;quot;TiDB&amp;quot;为例 （1）构造Key Range:一个表中所有的RowID都在[0,MaxInt64]这个范围内，那么我们可以根据Row Key编码规则，构造出一个[StartKey,EndKey]这样的左开右闭的区间 （2）根据上面构造出的Key Range。读取TiKV中的数据 （3）过滤数据，对于读取到的每一行的数据，计算name = &amp;quot;TiDB&amp;quot;这个表达式。，如果为真，向上返回这一行，否则丢弃这一行 （4）根据返回的行数计算Count值
总结该方案的缺点： （1）从TiKV中读取每一行数据时都要进行一次扫描，但是每次扫描都是一次RPC调用，当扫描的数据很多时，这个开销非常大 （2）不满足条件的行可以不用读取 （3）只需统计行数即可 2、方案的改进：分布式SQL的运算
将计算、Filter、聚合函数、GroupBy尽量地靠近存储节点，以避免大量的RPC调用 数据逐层返回的示意图如下： 3、SQL层的架构 TiDB的SQL层是非常复杂的 （1）用户的请求会直接或者通过Load Balancer发送到TiDB-Server, （2）TiDB-Server会解析MySQL Protocol Packet，获取请求的内容，然后左语法解析、查询特定和优化，执行查询计划和处理数据 (3) 数据全部存储在TiKV的集群中，TiDB和TiKV-Server交互，获取数据 （4）最后TiDB-Server需要将特定的查询结果返回给特定的用户</description>
    </item>
    
    <item>
      <title>TiDB架构原理简介</title>
      <link>https://sin-coder.github.io/database/tidbinfr/</link>
      <pubDate>Fri, 24 Apr 2020 00:52:37 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/database/tidbinfr/</guid>
      <description>TiDB集群主要包括三个核心组件：TiDB Server,PD Server,TiKV Server TiSpark组件(解决用户复杂OLAP需求)和TiDB Operator组件(简化云上部署管理) 架构简介: 一、TiDB Server TiDB Server 负责接收SQL请求，处理SQL相关的逻辑，并通过PD找到存储计算所需数据的TiKV地址，与TiKV交互数据，最终返回结果。 TiDB Server 是无状态的，其本身并不存储数据，只负责计算，可以无限水平扩展，可通过负载均衡的组件(LVS、HAProxy或者F5) 对外提供统一的接入地址 二、PD Server(Placement Driver)是整个集群的管理模块，具体的工作职能是： 1、存储这个集群的原信息(某个Key具体存储在哪个TiKV的节点) 2、对TiKV集群进行调度和负载均衡(数据迁移、Raft Group leader) 3、分配全局唯一且递增的事物ID PD通过Raft协议保证数据的安全性,Raft的leader Server负责处理所有操作，其余的PD Server仅用于保证高可用性 一般情况下建议部署奇数个节点 三、TiKV Server TiKV Server是TiDB中的数据存储引擎，存储的数据的基本单位是Region TiKV使用Raft做协议复制，保证数据的一致性和容灾。副本以Region为单位进行管理，不同节点上的多个Region构成一个Raft Group，互为副本 数据在多个TiKV之间的负载均衡由PD调度，也是以Region为基本单位的
四、TiSpark 主要应用在OLTP和OLAP中 五、TiDB Opeartor 提供在主流云基础设施上部署管理TiDB集群的能力 集成一键部署、多集群混部、自动运维、故障自愈  特性分析： TiDB的两大核心特点就是无限水平扩展和高可用性 一、无限水平扩展 水平扩展包括计算能力和存储能力。TiDB Server负责处理Server的请求，可以增加TiDB Server节点，提高计算能力 TiKV负责存储数据，增加TiKV可以增加存储能力 二、高可用性 TiDB、TiKV和PD这三个组件都能容忍部分实例失效，不影响整个集群的可用性; 1、TiDB TiDB是无状态的,应该部署至少两个实例，前端通过负载均衡的组件对外提供服务 如果单次请求失败后，重新连接就可继续获得服务 2、PD PD是一个集群，通过Raft协议保持数据的一致性，当单个实例失效时，如果不是Raft的Leader，完全不影响服务 如果是Raft的Leader，会重新选举出Raft的leader,自动恢复服务 但是PD在选举的过程中无法对外提供服务 3、TiKV 也是一个集群，通过Raft协议保证数据的一致性，副本数量可配置，默认保持三副本，通过PD做负载均衡调度。 单个节点失效时，会影响这个节点上存储的所有Region。 对于Region中的Leader节点，会中断服务，等待重新选举；对于Follower节点，不会影响服务 如果某个TiKV节点失效时，并且在一段时间内无法恢复，PD会将其上的数据迁移到其他的TiKV节点上</description>
    </item>
    
    <item>
      <title>TiDB数据库简介</title>
      <link>https://sin-coder.github.io/database/tidbintro/</link>
      <pubDate>Fri, 24 Apr 2020 00:51:14 +0800</pubDate>
      
      <guid>https://sin-coder.github.io/database/tidbintro/</guid>
      <description>TiDB主要用来应用在全部的OLTP的场景和大部分的OLAP场景 TiDB作为一个开源的分布式数据库主要有以下特点： 1. 高度兼容MySQL MySQL迁移到TiDB是非常容易的，无论是单机迁移还是集群的迁移 2. 支持无限的水平弹性扩展 新增节点实现TiDB的水平扩展，轻松地应对高并发、海量数据的场景 3. 强一致性和高可用 基于Raft的多数派选举协议可以提供100%的数据强一致性，可以实现故障的自动恢复 4. 分布式事务 支持ACID事务</description>
    </item>
    
  </channel>
</rss>